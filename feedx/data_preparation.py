# Copyright 2023 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Data preparation for FeedX.

This module contains functions for loading and preparing the data required for
FeedX.
"""

import datetime as dt
from typing import Callable
import numpy as np
import pandas as pd

GOOGLE_ADS_PERFORMANCE_CSV_COLUMNS = {
    "date_column": "Week",
    "item_id_column": "Item ID",
    "impressions_column": "Impr.",
    "clicks_column": "Clicks",
}
GOOGLE_ADS_PERFORMANCE_READ_CSV_ARGS = {"header": 2, "thousands": ","}


def generate_historical_synthetic_data(
    n_items: int = 7000,
    avg_impressions: int = 100,
    std_impressions: int = 100,
    avg_ctr: float = 0.02,
    std_ctr: float = 0.015,
    historical_days: int = 90,
    seed: int | None = None,
) -> pd.DataFrame:
  """Generates historical synthetic experiment data automatically.

  If users don't provide their own historical data, this option allows them to
  see examples of experiment designs before providing their data.

  Use lognormal distribution for expected control impressions. It is suitable
  for real-world impressions that should only be >=0 and tend to typically have
  a positive skew. Once expected control impressions is calculated, use it as
  the lambda in a poisson distribution to simulate the amount of impressions
  that are expected during a given time frame.

  Use beta distribution for expected control CTR because it is defined over the
  interval [0,1], which makes it a good candidate for measuring ratio KPIs. Once
  expected control CTR is calculated, use it as the probability in a binomial
  distribution and the number of impressions calculated previously as the sample
  size to simulate the amount of clicks expected.

  Args:
    n_items: sample size
    avg_impressions: average impressions for the sample
    std_impressions: standard deviation of impressions for the sample
    avg_ctr: average CTR for the sample
    std_ctr: standard deviation of CTR for the sample
    historical_days: amount of days in historical data
    seed: seed for random number generator

  Returns:
    Dataframe with synthetic data generated by FeedX.

  Raises:
    ValueError: If avg_ctr less than 0 or greater than 1 and if std_ctr is less
      than or equal to zero or greater than 1.
  """
  valid_avg_ctr = avg_ctr >= 0 and avg_ctr <= 1
  valid_std_ctr = std_ctr > 0 and std_ctr <= 1
  if not valid_avg_ctr:
    raise ValueError("avg_ctr must be between 0 and 1")
  if not valid_std_ctr:
    raise ValueError("std_ctr must be between 0 and 1")

  date_fmt = "%Y-%m-%d"
  today = dt.date.today()
  dates = [
      (today - dt.timedelta(days=i + 1)).strftime(date_fmt)
      for i in range(historical_days)
  ]
  item_ids = [f"items_{i}" for i in range(n_items)]

  dates_matrix, item_ids_matrix = np.meshgrid(dates, item_ids)

  shape = dates_matrix.shape

  rng = np.random.default_rng(seed)

  mean = np.log(avg_impressions) - 0.5 * np.log(
      std_impressions**2 / avg_impressions**2 + 1
  )
  sigma = np.sqrt(np.log(std_impressions**2 / avg_impressions**2 + 1))
  expected_impressions_control = rng.lognormal(
      mean=mean, sigma=sigma, size=n_items
  ).reshape(-1, 1)

  beta_dist_params = np.sqrt(avg_ctr * (1.0 - avg_ctr)) / std_ctr
  expected_ctr_control = rng.beta(
      a=avg_ctr * beta_dist_params,
      b=(1.0 - avg_ctr) * beta_dist_params,
      size=n_items,
  ).reshape(-1, 1)

  impressions = rng.poisson(lam=expected_impressions_control, size=shape)

  clicks = rng.binomial(n=impressions, p=expected_ctr_control)

  data = pd.DataFrame({
      "date": dates_matrix.flatten(),
      "item_id": item_ids_matrix.flatten(),
      "impressions": impressions.flatten(),
      "clicks": clicks.flatten(),
  })

  return data


def standardize_column_names_and_types(
    data: pd.DataFrame,
    *,
    item_id_column: str,
    date_column: str,
    clicks_column: str | None = None,
    impressions_column: str | None = None,
    total_cost_column: str | None = None,
    conversions_column: str | None = None,
    total_conversion_value_column: str | None = None,
    custom_columns: dict[str, str] | None = None,
    custom_parse_functions: (
        dict[str, Callable[[pd.Series], pd.Series]] | None
    ) = None,
) -> pd.DataFrame:
  """Standardizes the column names and types.

  This renames the column names to their standardized names, casts the columns
  to a standardized data type, and drops any other columns. The item_id and
  date columns are requied, but all others are optional. However typically at
  least one other metric column is needed downstream.

  The custom_columns and custom_parse_functions are there to allow the user to
  include their own custom metrics, but they should not overlap the standard
  columns.

  Args:
    data: synthetic, historical, or runtime data
    item_id_column: The name of the column containing the unique identifier of
      the item in data.
    date_column: The name of the date column in data.
    clicks_column: The name of the clicks column in data if it exists, otherwise
      None. Defaults to None.
    impressions_column: The name of the impressions column in data if it exists,
      otherwise None. Defaults to None.
    total_cost_column: The name of the column containing the total cost (spend)
      in data if it exists, otherwise None. Defaults to None.
    conversions_column: The name of the conversions column in data if it exists,
      otherwise None. Defaults to None.
    total_conversion_value_column: The name of the column containing the total
      conversion value in data if it exists, otherwise None. Defaults to None.
    custom_columns: Key value pairs, where the key is the output column name and
      the value is the input column name, for any extra columns that have not
      already been specified.
    custom_parse_functions: Optionally, a dictionary containing functions to
      cast the custom columns to the correct data type. The keys are the
      standard column names, and the values are the functions which should take
      as an input a pandas series and return a pandas series.

  Returns:
    Dataframe with standardized column names and types.

  Raises:
    ValueError: If any of the custom columns overlap with the standard columns,
      or if any of the custom_parse_functions don't exist in the custom columns.
    ValueError: If the data does not contain the required columns.
  """
  custom_columns = custom_columns or {}
  custom_parse_functions = custom_parse_functions or {}

  parse_functions = {
      "item_id": lambda x: x.astype(str),
      "date": pd.to_datetime,
      "clicks": lambda x: pd.to_numeric(x).astype(int),
      "impressions": lambda x: pd.to_numeric(x).astype(int),
      "total_cost": lambda x: pd.to_numeric(x).astype(float),
      "conversions": lambda x: pd.to_numeric(x).astype(int),
      "total_conversion_value": lambda x: pd.to_numeric(x).astype(float),
  }
  column_names = {
      "item_id": item_id_column,
      "date": date_column,
      "clicks": clicks_column,
      "impressions": impressions_column,
      "total_cost": total_cost_column,
      "conversions": conversions_column,
      "total_conversion_value": total_conversion_value_column,
  }

  for custom_column in custom_columns.keys():
    if custom_column in column_names.keys():
      raise ValueError(
          f"The custom column {custom_column} is one of the standard columns,"
          f" set it with the {custom_column}_column argument."
      )

  for custom_parse_function in custom_parse_functions.keys():
    if custom_parse_function in parse_functions.keys():
      raise ValueError(
          f"The custom parse function {custom_parse_function} is one of the"
          " standard parse functions, no need to set it."
      )
    if custom_parse_function not in custom_columns.keys():
      raise ValueError(
          f"The custom parse function {custom_parse_function} must also be set"
          " in custom_columns."
      )

  parse_functions = parse_functions | custom_parse_functions
  column_names = column_names | custom_columns

  column_names = {
      parsed_name: input_name
      for parsed_name, input_name in column_names.items()
      if input_name is not None
  }
  parse_functions = {
      parsed_name: function
      for parsed_name, function in parse_functions.items()
      if parsed_name in column_names.keys()
  }
  column_renamer = {
      input_name: parsed_name
      for parsed_name, input_name in column_names.items()
  }

  for input_column in column_names.values():
    if input_column not in data.columns:
      raise ValueError(
          f"The input column {input_column} does not exist in the data."
      )

  output_data = data[list(column_names.values())].copy()
  output_data.rename(column_renamer, axis=1, inplace=True)

  for column in output_data.columns:
    parse_function = parse_functions.get(column, lambda x: x)
    output_data[column] = parse_function(output_data[column])

  return output_data


def fill_missing_rows_with_zeros(
    data: pd.DataFrame,
    item_id_column: str = "item_id",
    date_column: str = "date",
) -> pd.DataFrame:
  """Adds any missing rows in the dataframe with zeros.

  Often the input data is missing the rows where the item got 0 for all the
  metrics. However, this will break some of the analysis functions, so this
  function makes sure that there is at least one row for every unique
  combination of item_id and date.

  Args:
    data: synthetic, historical, or runtime data
    item_id_column: The column name containing the item id. Defaults to
      "item_id".
    date_column: The column name containing the date. Defaults to "date".

  Returns:
    Data columns in specified data type with missing date / item_id combinations
    filled with zeros.

  Raises:
    ValueError: If the item_id_column or date_column do not exist in the data.
  """
  required_columns = {item_id_column, date_column}
  missing_columns = required_columns - set(data.columns.values)
  if missing_columns:
    raise ValueError(
        "The data is missing the following required columns: "
        f"{missing_columns}."
    )

  all_items = data[[item_id_column]].drop_duplicates()
  all_dates = data[[date_column]].drop_duplicates()
  all_item_date_combinations = all_items.merge(all_dates, how="cross")
  output_data = all_item_date_combinations.merge(
      data, on=[item_id_column, date_column], how="left"
  )

  metric_columns = [
      column
      for column in output_data.columns
      if column not in [date_column, item_id_column]
  ]
  for metric_column in metric_columns:
    original_dtype = data[metric_column].dtype
    output_data[metric_column] = (
        output_data[metric_column].fillna(0).astype(original_dtype)
    )

  return output_data


def downsample_items(
    data: pd.DataFrame,
    downsample_fraction: float,
    item_id_column: str,
    rng: np.random.RandomState,
) -> pd.DataFrame:
  """Downsamples the items in the data.

  This will sample downsample_fraction of the items in the original data without
  replacement. If downsample_fraction = 1, it will return the original data
  unchanged.

  Args:
    data: The data to be downsampled.
    downsample_fraction: The fraction of items to select from the data, must be
      between 0 and 1. If 1, all items are returned.
    item_id_column: The column in the data containing the item identifier.
    rng: The random state used to ensure sampling is reproducable.

  Returns:
    Downsampled data.

  Raises:
    ValueError: If the downsample fraction is not in the range (0.0, 1.0], or
      if the item_id_column is not in the data.
  """
  if (downsample_fraction <= 0.0) | (downsample_fraction > 1.0):
    raise ValueError("Downsample_fraction must be in the range of (0.0, 1.0].")

  if item_id_column not in data.columns:
    raise ValueError(f"The column {item_id_column} does not exist in the data.")

  print(
      f"Data has {len(data.index.values)} rows and"
      f" {data[item_id_column].nunique()} items."
  )

  if downsample_fraction == 1.0:
    print("Not downsampling, since the downsample_fraction = 1.0")
    return data

  print(
      f"Sampling {downsample_fraction:.2%} of the items in the original data."
  )
  sampled_items = (
      data[item_id_column]
      .drop_duplicates()
      .sample(frac=downsample_fraction, random_state=rng)
  )
  data = data[data[item_id_column].isin(sampled_items)]
  print(
      f"Downsampled data has {len(data.index.values)} rows and"
      f" {data[item_id_column].nunique()} items."
  )
  return data


def _validate_every_value_exists_exactly_once_for_every_group(
    data: pd.DataFrame,
    *,
    value_column: str,
    group_column: str,
) -> None:
  """Validates that every value exists exactly once in every group."""
  value_counts_per_group = (
      data[[group_column, value_column]]
      .groupby(group_column)
      .count()[value_column]
  )

  unique_value_counts_per_group = (
      data[[group_column, value_column]]
      .groupby(group_column)
      .nunique()[value_column]
  )

  if np.any(value_counts_per_group != unique_value_counts_per_group):
    bad_groups = value_counts_per_group.loc[
        value_counts_per_group != unique_value_counts_per_group
    ].index.values
    raise ValueError(
        f"There are duplicate {value_column} when {group_column} in "
        f"{bad_groups}"
    )

  value_counts_first_group = value_counts_per_group.values[0]
  if np.any(value_counts_per_group.values != value_counts_first_group):
    raise ValueError(
        f"Some {group_column} are missing {value_column}. Below are the number "
        f"of unique {value_column} per {group_column}, which should be equal "
        f"for all dates.\n{value_counts_per_group}"
    )
  else:
    print(
        f"All {group_column} have {value_counts_first_group:,} {value_column},"
        " check passed."
    )


def validate_historical_data(
    historical_data: pd.DataFrame,
    item_id_column: str,
    date_column: str,
) -> None:
  """Runs all the required validation for the historical data.

  The historical data is used to perform simulations to design the optimal
  experiment. This validates that:

  - All items have exactly 1 row for every date, there are no missing
    date / item combinations or duplicates.

  Args:
    historical_data: The historical data to be validated.
    item_id_column: The column in the data contining the item identifier.
    date_column: The column in the data containing the date.

  Raises:
    ValueError: If any of the validations fail.
  """

  _validate_every_value_exists_exactly_once_for_every_group(
      historical_data,
      group_column=date_column,
      value_column=item_id_column,
  )
