# Copyright 2023 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Data preparation for FeedX.

This module contains functions for loading and preparing the data required for
FeedX.
"""

import datetime as dt
from typing import Callable
import numpy as np
import pandas as pd


def generate_historical_synthetic_data(
    n_items: int = 7000,
    avg_impressions: int = 100,
    std_impressions: int = 100,
    avg_ctr: float = 0.02,
    std_ctr: float = 0.015,
    historical_days: int = 90,
    seed: int | None = None,
) -> pd.DataFrame:
  """Generates historical synthetic experiment data automatically.

  If users don't provide their own historical data, this option allows them to
  see examples of experiment designs before providing their data.

  Use lognormal distribution for expected control impressions. It is suitable
  for real-world impressions that should only be >=0 and tend to typically have
  a positive skew. Once expected control impressions is calculated, use it as
  the lambda in a poisson distribution to simulate the amount of impressions
  that are expected during a given time frame.

  Use beta distribution for expected control CTR because it is defined over the
  interval [0,1], which makes it a good candidate for measuring ratio KPIs. Once
  expected control CTR is calculated, use it as the probability in a binomial
  distribution and the number of impressions calculated previously as the sample
  size to simulate the amount of clicks expected.

  Args:
    n_items: sample size
    avg_impressions: average impressions for the sample
    std_impressions: standard deviation of impressions for the sample
    avg_ctr: average CTR for the sample
    std_ctr: standard deviation of CTR for the sample
    historical_days: amount of days in historical data
    seed: seed for random number generator

  Returns:
    Dataframe with synthetic data generated by FeedX.

  Raises:
    ValueError: If avg_ctr less than 0 or greater than 1 and if std_ctr is less
      than or equal to zero or greater than 1.
  """
  valid_avg_ctr = avg_ctr >= 0 and avg_ctr <= 1
  valid_std_ctr = std_ctr > 0 and std_ctr <= 1
  if not valid_avg_ctr:
    raise ValueError("avg_ctr must be between 0 and 1")
  if not valid_std_ctr:
    raise ValueError("std_ctr must be between 0 and 1")

  date_fmt = "%Y-%m-%d"
  today = dt.date.today()
  dates = [
      (today - dt.timedelta(days=i + 1)).strftime(date_fmt)
      for i in range(historical_days)
  ]
  item_ids = [f"items_{i}" for i in range(n_items)]

  dates_matrix, item_ids_matrix = np.meshgrid(dates, item_ids)

  shape = dates_matrix.shape

  rng = np.random.default_rng(seed)

  mean = np.log(avg_impressions) - 0.5 * np.log(
      std_impressions**2 / avg_impressions**2 + 1
  )
  sigma = np.sqrt(np.log(std_impressions**2 / avg_impressions**2 + 1))
  expected_impressions_control = rng.lognormal(
      mean=mean, sigma=sigma, size=n_items
  ).reshape(-1, 1)

  beta_dist_params = np.sqrt(avg_ctr * (1.0 - avg_ctr)) / std_ctr
  expected_ctr_control = rng.beta(
      a=avg_ctr * beta_dist_params,
      b=(1.0 - avg_ctr) * beta_dist_params,
      size=n_items,
  ).reshape(-1, 1)

  impressions = rng.poisson(lam=expected_impressions_control, size=shape)

  clicks = rng.binomial(n=impressions, p=expected_ctr_control)

  data = pd.DataFrame({
      "date": dates_matrix.flatten(),
      "item_id": item_ids_matrix.flatten(),
      "impressions": impressions.flatten(),
      "clicks": clicks.flatten(),
  })

  return data


def load_historical_data_from_csv(
    file: str,
    date_column_name: str = "Week",
    product_id_column_name: str = "Item ID",
    impressions_column_name: str = "Impr.",
    clicks_column_name: str = "Clicks",
) -> pd.DataFrame:
  """Reads historical data from a csv file.

  Specifically, it's designed to conform to format of "Shopping - Item ID"
  Report provided by Google Ads GUI Columns are assumed to include: Week Start,
  Item ID, Clicks and Impressions.

  Args:
    file: file to read CSV from (passed to Pandas)
    date_column_name: name of week start column
    product_id_column_name: name of Item ID column
    impressions_column_name: contains impressions for product & week
    clicks_column_name: contains clicks for product & week

  Returns:
    DataFrame containing the data.
  """

  data = pd.read_csv(file, header=2, thousands=",")
  data = data[[
      date_column_name,
      product_id_column_name,
      impressions_column_name,
      clicks_column_name,
  ]].rename(
      {
          date_column_name: "date",
          product_id_column_name: "product_id",
          impressions_column_name: "impressions",
          clicks_column_name: "clicks",
      },
      axis=1,
  )

  return data


def standardize_column_names_and_types(
    data: pd.DataFrame,
    *,
    item_id_column: str,
    date_column: str,
    clicks_column: str | None = None,
    impressions_column: str | None = None,
    total_cost_column: str | None = None,
    conversions_column: str | None = None,
    total_conversion_value_column: str | None = None,
    custom_columns: dict[str, str] | None = None,
    custom_parse_functions: (
        dict[str, Callable[[pd.Series], pd.Series]] | None
    ) = None,
) -> pd.DataFrame:
  """Standardizes the column names and types.

  This renames the column names to their standardized names, casts the columns
  to a standardized data type, and drops any other columns. The item_id and
  date columns are requied, but all others are optional. However typically at
  least one other metric column is needed downstream.

  The custom_columns and custom_parse_functions are there to allow the user to
  include their own custom metrics, but they should not overlap the standard
  columns.

  Args:
    data: synthetic, historical, or runtime data
    item_id_column: The name of the column containing the unique identifier of
      the item in data.
    date_column: The name of the date column in data.
    clicks_column: The name of the clicks column in data if it exists, otherwise
      None. Defaults to None.
    impressions_column: The name of the impressions column in data if it exists,
      otherwise None. Defaults to None.
    total_cost_column: The name of the column containing the total cost (spend)
      in data if it exists, otherwise None. Defaults to None.
    conversions_column: The name of the conversions column in data if it exists,
      otherwise None. Defaults to None.
    total_conversion_value_column: The name of the column containing the total
      conversion value in data if it exists, otherwise None. Defaults to None.
    custom_columns: Key value pairs, where the key is the output column name and
      the value is the input column name, for any extra columns that have not
      already been specified.
    custom_parse_functions: Optionally, a dictionary containing functions to
      cast the custom columns to the correct data type. The keys are the
      standard column names, and the values are the functions which should take
      as an input a pandas series and return a pandas series.

  Returns:
    Dataframe with standardized column names and types.

  Raises:
    ValueError: If any of the custom columns overlap with the standard columns,
      or if any of the custom_parse_functions don't exist in the custom columns.
    ValueError: If the data does not contain the required columns.
  """
  custom_columns = custom_columns or {}
  custom_parse_functions = custom_parse_functions or {}

  parse_functions = {
      "item_id": lambda x: x.astype(str),
      "date": pd.to_datetime,
      "clicks": lambda x: pd.to_numeric(x).astype(int),
      "impressions": lambda x: pd.to_numeric(x).astype(int),
      "total_cost": lambda x: pd.to_numeric(x).astype(float),
      "conversions": lambda x: pd.to_numeric(x).astype(int),
      "total_conversion_value": lambda x: pd.to_numeric(x).astype(float),
  }
  column_names = {
      "item_id": item_id_column,
      "date": date_column,
      "clicks": clicks_column,
      "impressions": impressions_column,
      "total_cost": total_cost_column,
      "conversions": conversions_column,
      "total_conversion_value": total_conversion_value_column,
  }

  for custom_column in custom_columns.keys():
    if custom_column in column_names.keys():
      raise ValueError(
          f"The custom column {custom_column} is one of the standard columns,"
          f" set it with the {custom_column}_column argument."
      )

  for custom_parse_function in custom_parse_functions.keys():
    if custom_parse_function in parse_functions.keys():
      raise ValueError(
          f"The custom parse function {custom_parse_function} is one of the"
          " standard parse functions, no need to set it."
      )
    if custom_parse_function not in custom_columns.keys():
      raise ValueError(
          f"The custom parse function {custom_parse_function} must also be set"
          " in custom_columns."
      )

  parse_functions = parse_functions | custom_parse_functions
  column_names = column_names | custom_columns

  column_names = {
      parsed_name: input_name
      for parsed_name, input_name in column_names.items()
      if input_name is not None
  }
  parse_functions = {
      parsed_name: function
      for parsed_name, function in parse_functions.items()
      if parsed_name in column_names.keys()
  }
  column_renamer = {
      input_name: parsed_name
      for parsed_name, input_name in column_names.items()
  }

  for input_column in column_names.values():
    if input_column not in data.columns:
      raise ValueError(
          f"The input column {input_column} does not exist in the data."
      )

  output_data = data[list(column_names.values())].copy()
  output_data.rename(column_renamer, axis=1, inplace=True)

  for column in output_data.columns:
    parse_function = parse_functions.get(column, lambda x: x)
    output_data[column] = parse_function(output_data[column])

  return output_data


def fill_missing_rows_with_zeros(
    data: pd.DataFrame,
    item_id_column: str = "item_id",
    date_column: str = "date",
) -> pd.DataFrame:
  """Adds any missing rows in the dataframe with zeros.

  Often the input data is missing the rows where the item got 0 for all the
  metrics. However, this will break some of the analysis functions, so this
  function makes sure that there is at least one row for every unique
  combination of item_id and date.

  Args:
    data: synthetic, historical, or runtime data
    item_id_column: The column name containing the item id. Defaults to
      "item_id".
    date_column: The column name containing the date. Defaults to "date".

  Returns:
    Data columns in specified data type with missing date / item_id combinations
    filled with zeros.

  Raises:
    ValueError: If the item_id_column or date_column do not exist in the data.
  """
  required_columns = {item_id_column, date_column}
  missing_columns = required_columns - set(data.columns.values)
  if missing_columns:
    raise ValueError(
        "The data is missing the following required columns: "
        f"{missing_columns}."
    )

  all_items = data[[item_id_column]].drop_duplicates()
  all_dates = data[[date_column]].drop_duplicates()
  all_item_date_combinations = all_items.merge(all_dates, how="cross")
  output_data = all_item_date_combinations.merge(
      data, on=[item_id_column, date_column], how="left"
  )

  metric_columns = [
      column
      for column in output_data.columns
      if column not in [date_column, item_id_column]
  ]
  for metric_column in metric_columns:
    original_dtype = data[metric_column].dtype
    output_data[metric_column] = (
        output_data[metric_column].fillna(0).astype(original_dtype)
    )

  return output_data
