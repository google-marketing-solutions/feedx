# Copyright 2023 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Data preparation for FeedGen A/B Testing.

This file formats and validates data in preparation for A/B testing.
Users can test with synthetic data or upload their product performance data.
"""

import datetime as dt
from typing import Dict
import numpy as np
import pandas as pd


def generate_historical_synthetic_data(
    n_items: int = 7000,
    avg_impressions: int = 100,
    std_impressions: int = 100,
    avg_ctr: float = 0.02,
    std_ctr: float = 0.015,
    historical_days: int = 90,
    seed: int | None = None,
) -> pd.DataFrame:
  """Generates historical synthetic experiment data automatically.

  If users don't provide their own historical data, this option allows them to
  see examples of experiment designs before providing their data.

  Use lognormal distribution for expected control impressions. It is suitable
  for real-world impressions that should only be >=0 and tend to typically have
  a positive skew. Once expected control impressions is calculated, use it as
  the lambda in a poisson distribution to simulate the amount of impressions
  that are expected during a given time frame.

  Use beta distribution for expected control CTR because it is defined over the
  interval [0,1], which makes it a good candidate for measuring ratio KPIs. Once
  expected control CTR is calculated, use it as the probability in a binomial
  distribution and the number of impressions calculated previously as the sample
  size to simulate the amount of clicks expected.

  Args:
    n_items: sample size
    avg_impressions: average impressions for the sample
    std_impressions: standard deviation of impressions for the sample
    avg_ctr: average CTR for the sample
    std_ctr: standard deviation of CTR for the sample
    historical_days: amount of days in historical data
    seed: seed for random number generator

  Returns:
    Dataframe with synthetic data generated by FeedX.

  Raises:
    ValueError: If avg_ctr less than 0 or greater than 1 and if std_ctr is less
      than or equal to zero or greater than 1.
  """
  valid_avg_ctr = avg_ctr >= 0 and avg_ctr <= 1
  valid_std_ctr = std_ctr > 0 and std_ctr <= 1
  if not valid_avg_ctr:
    raise ValueError("avg_ctr must be between 0 and 1")
  if not valid_std_ctr:
    raise ValueError("std_ctr must be between 0 and 1")

  date_fmt = "%Y-%m-%d"
  today = dt.date.today()
  dates = [
      (today - dt.timedelta(days=i + 1)).strftime(date_fmt)
      for i in range(historical_days)
  ]
  item_ids = [f"items_{i}" for i in range(n_items)]

  dates_matrix, item_ids_matrix = np.meshgrid(dates, item_ids)

  shape = dates_matrix.shape

  rng = np.random.default_rng(seed)

  mean = np.log(avg_impressions) - 0.5 * np.log(
      std_impressions**2 / avg_impressions**2 + 1
  )
  sigma = np.sqrt(np.log(std_impressions**2 / avg_impressions**2 + 1))
  expected_impressions_control = rng.lognormal(
      mean=mean, sigma=sigma, size=n_items
  ).reshape(-1, 1)

  beta_dist_params = np.sqrt(avg_ctr * (1.0 - avg_ctr)) / std_ctr
  expected_ctr_control = rng.beta(
      a=avg_ctr * beta_dist_params,
      b=(1.0 - avg_ctr) * beta_dist_params,
      size=n_items,
  ).reshape(-1, 1)

  impressions = rng.poisson(lam=expected_impressions_control, size=shape)

  clicks = rng.binomial(n=impressions, p=expected_ctr_control)

  data = pd.DataFrame({
      "date": dates_matrix.flatten(),
      "item_id": item_ids_matrix.flatten(),
      "impressions": impressions.flatten(),
      "clicks": clicks.flatten(),
  })

  return data


def load_historical_data_from_csv(
    file: str,
    date_column_name: str = "Week",
    product_id_column_name: str = "Item ID",
    impressions_column_name: str = "Impr.",
    clicks_column_name: str = "Clicks",
) -> pd.DataFrame:
  """Reads historical data from a csv file.

  Specifically, it's designed to conform to format of "Shopping - Item ID"
  Report provided by Google Ads GUI Columns are assumed to include: Week Start,
  Item ID, Clicks and Impressions.

  Args:
    file: file to read CSV from (passed to Pandas)
    date_column_name: name of week start column
    product_id_column_name: name of Item ID column
    impressions_column_name: contains impressions for product & week
    clicks_column_name: contains clicks for product & week

  Returns:
    DataFrame containing the data.
  """

  data = pd.read_csv(file, header=2, thousands=",")
  data = data[[
      date_column_name,
      product_id_column_name,
      impressions_column_name,
      clicks_column_name,
  ]].rename(
      {
          date_column_name: "date",
          product_id_column_name: "product_id",
          impressions_column_name: "impressions",
          clicks_column_name: "clicks",
      },
      axis=1,
  )

  return data


def standardize_historical_column_names(
    data: pd.DataFrame,
    date_column_name: str,
    item_id_column_name: str,
    clicks_column_name: str,
    impressions_column_name: str,
) -> pd.DataFrame:
  """Function to rename historical data columns.

  Args:
    data: synthetic, historical, or runtime data
    date_column_name: name of week start column
    item_id_column_name: name of Item ID column
    clicks_column_name: contains clicks for product & week
    impressions_column_name: contains impressions for product & week

  Returns:
    Dataframe with standardized column names.
  """
  data = data[[
      date_column_name,
      item_id_column_name,
      clicks_column_name,
      impressions_column_name,
  ]].rename(
      {
          date_column_name: "date",
          item_id_column_name: "item_id",
          clicks_column_name: "clicks",
          impressions_column_name: "impressions",
      },
      axis=1,
  )

  return data


def parse_data(
    data: pd.DataFrame,
    columns: Dict[str, str]
    ) -> pd.DataFrame:
  """Format the loaded data.

  Assign data types to user-provided data and remove duplicates.

  Args:
    data: synthetic, historical, or runtime data
    columns: columns provided in the data sets that contain date, item_id, and
      metrics such as clicks, impressions, and conversions. Not all users will
      provide the same set of metrics, so this argument is intended to pass
      what is provided and apply the corresponding formatting. The keys of the 
      dict are the names of the input columns and their values are the 
      standardized output column names

  Returns:
    Data columns in specified data type with duplicates removed.
  
  Raises:
  ValueError: If the value of `output_column` is not valid.
  ValueError: If the value of `input_column` is not valid.
  """

  _parse_function = {
      "item_id": lambda x: x.astype(str),
      "date": lambda x: pd.to_datetime(x),
      "clicks": lambda x: pd.to_numeric(x).astype(int),
      "impressions": lambda x: pd.to_numeric(x).astype(int),
      "total_cost": lambda x: pd.to_numeric(x).astype(float),
      "conversions": lambda x: pd.to_numeric(x).astype(int),
      "total_conversion_value": lambda x: pd.to_numeric(x).astype(float)
  }

  for output_column in columns.values():
    if output_column not in _parse_function.keys():
      raise ValueError(f"The output column {output_column} is not expected.")

  for input_column in columns.keys():
    if input_column not in data.columns:
      raise ValueError(
          f"The input column {input_column} does not exist in the data.")

  output_data = data[list(columns.keys())]
  output_data.rename(columns, axis=1, inplace=True)

  for column in output_data.columns:
    output_data[column] = _parse_function[column](output_data[column])

  all_products = output_data[["item_id"]].drop_duplicates()
  all_dates = output_data[["date"]].drop_duplicates()
  all_product_date_combinations = all_products.merge(all_dates, how="cross")

  data = all_product_date_combinations.merge(
      output_data, on=["item_id", "date"], how="left").fillna(0)

  return data
